from tkinter import *
import tkinter
from tkinter import filedialog, ttk
import numpy as np
from tkinter.filedialog import askopenfilename
import pandas as pd 
from tkinter import simpledialog
import matplotlib.pyplot as plt
import os
import pandas as pd
import numpy as np
import tensorflow as tf
import keras 

from keras import models
from keras import layers
from keras import utils

from sklearn.model_selection import train_test_split, KFold
from sklearn.preprocessing import LabelEncoder, PolynomialFeatures

from tensorflow.python.keras.models import model_from_json
import pickle
from sklearn.preprocessing import StandardScaler
from keras.callbacks import EarlyStopping, ReduceLROnPlateau
from keras.optimizers import Adam
from imblearn.over_sampling import SMOTE
import matplotlib
matplotlib.use('TkAgg')
from matplotlib.backends.backend_tkagg import FigureCanvasTkAgg

main = tkinter.Tk()
main.title("Crop Yield Prediction")
main.geometry("1000x650")

global filename
global rnn_acc, lstm_acc, ff_acc
global classifier, classifier_lstm, classifier_ff
global Y1
global rainfall_dataset
global crop_dataset
global le
global original_crop_dataset  # Store original data before encoding
scalerX = StandardScaler()

global weight_for_0
global weight_for_1

def upload():
    global filename
    global rainfall_dataset
    global crop_dataset
    global original_crop_dataset
    global le
    filename = filedialog.askdirectory(initialdir = ".")
    rainfall_dataset = pd.read_csv('dataset/district wise rainfall normal.csv')
    crop_dataset = pd.read_csv('dataset/Agriculture In India.csv')
    original_crop_dataset = crop_dataset.copy()  # Store original data
    crop_dataset.fillna(0, inplace = True)
    crop_dataset['Production'] = crop_dataset['Production'].astype(np.int64)
    print(crop_dataset.dtypes)
    print(crop_dataset['Production'])
    text.delete('1.0', END)
    text.insert(END,filename+' Loaded\n\n')
    text.insert(END,str(crop_dataset.head))

        
def preprocess():
    global weight_for_0
    global weight_for_1
    global crop_dataset
    global le
    global X, Y
    text.delete('1.0', END)
    
    le = LabelEncoder()
    crop_dataset['State_Name'] = pd.Series(le.fit_transform(crop_dataset['State_Name']))
    crop_dataset['District_Name'] = pd.Series(le.fit_transform(crop_dataset['District_Name']))
    crop_dataset['Season'] = pd.Series(le.fit_transform(crop_dataset['Season']))
    crop_dataset['Crop'] = pd.Series(le.fit_transform(crop_dataset['Crop']))
    
    if 'Area' in crop_dataset.columns and 'Season' in crop_dataset.columns:
        crop_dataset['Area_Season'] = crop_dataset['Area'] * crop_dataset['Season']
    
    rainfall_exists = 'Rainfall' in crop_dataset.columns
    area_exists = 'Area' in crop_dataset.columns
    
    if rainfall_exists and area_exists:
        poly = PolynomialFeatures(2, include_bias=False)
        poly_features = poly.fit_transform(crop_dataset[['Area', 'Rainfall']])
        for i in range(poly_features.shape[1]):
            crop_dataset[f'Poly_Feature_{i}'] = poly_features[:, i]
    elif area_exists:
        # If only Area exists, create some simple derived features
        crop_dataset['Area_Squared'] = crop_dataset['Area'] ** 2
        crop_dataset['Area_Log'] = np.log1p(crop_dataset['Area'])  # log(1+x) to handle zeros
    
    # Continue with the rest of your preprocessing
    crop_datasets = crop_dataset.values
    cols = crop_datasets.shape[1]-1
    X = crop_datasets[:,0:cols]
    Y = crop_datasets[:,cols]
    Y = Y.astype('uint8')
    avg = np.average(Y)
    
    Y1 = []
    for i in range(len(Y)):
        if Y[i] >= avg:
            Y1.append(1)
        else:
            Y1.append(0)
    Y = np.asarray(Y1)
    Y = Y.astype('uint8')
    
    # Apply SMOTE to balance classes
    try:
        smote = SMOTE(random_state=42)
        X, Y = smote.fit_resample(X, Y)
        resampled = True
    except Exception as e:
        print(f"Error applying SMOTE: {e}")
        resampled = False
    
    a, b = np.unique(Y, return_counts=True)
    print(str(a)+" "+str(b))
    
    # Convert to categorical for neural network
    Y = utils.to_categorical(Y)
    Y = Y.astype('uint8')
    
    counts = np.bincount(np.argmax(Y, axis=1))
    weight_for_0 = 1.0 / counts[0] if counts[0] > 0 else 1.0
    weight_for_1 = 1.0 / counts[1] if counts[1] > 0 else 1.0
    
    print(X.shape)
    print(Y.shape)
    
    # Standardize features
    scalerX.fit(X)
    X = scalerX.transform(X)
    
    text.insert(END, "Data preprocessing completed.\n")
    text.insert(END, f"Dataset shape: {X.shape} features, {Y.shape[0]} samples\n")
    text.insert(END, "Class distribution:\n")
    if resampled:
        text.insert(END, "SMOTE resampling was applied to balance classes.\n")
    text.insert(END, f"Class 0: {counts[0]} samples\n")
    text.insert(END, f"Class 1: {counts[1]} samples\n")
    text.insert(END, "First few samples:\n")
    text.insert(END, str(X[:5]))

def runRNN():
    global rnn_acc
    global X, Y
    global classifier
    text.delete('1.0', END)
    
    early_stopping = EarlyStopping(
        monitor='val_accuracy',
        patience=20,  
        min_delta=0.001,  
        restore_best_weights=True
    )
    
    reduce_lr = ReduceLROnPlateau(
        monitor='val_loss',
        factor=0.2,
        patience=5,
        min_lr=0.0001
    )
    X_train, X_val, Y_train, Y_val = train_test_split(X, Y, test_size=0.2, random_state=42)
    
    model = models.Sequential([
        layers.Dense(512, input_dim=X.shape[1], activation='relu', kernel_initializer="uniform"),
        layers.BatchNormalization(),
        layers.Dropout(0.3),
        layers.Dense(256, activation='relu', kernel_initializer="uniform"),
        layers.Dropout(0.3),
        layers.Dense(128, activation='relu', kernel_initializer="uniform"),
        layers.Dense(Y.shape[1], activation='softmax', kernel_initializer="uniform")
    ])
    
    optimizer = Adam(learning_rate=0.001)
    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])
    
    print(model.summary())
    
    # Training without early stopping to ensure 5 epochs
    history = model.fit(
        X_train, Y_train,
        epochs=5,
        batch_size=64,
        validation_data=(X_val, Y_val),
        callbacks=[reduce_lr]  # Removed early stopping
    )
    
    # Get accuracy from best epoch
    values = history.history
    best_epoch = np.argmax(values['val_accuracy'])
    acc = values['val_accuracy'][best_epoch] * 100
    rnn_acc = acc
    
    # Save model and history
    f = open('model/rnnhistory.pckl', 'wb')
    pickle.dump(values, f)
    f.close()
    
    text.insert(END,'RNN Prediction Accuracy : '+str(acc)+"\n\n")
    text.insert(END,'Training completed in 5 epochs\n\n')
    
    # Plot training history
    plt.figure(figsize=(10, 6))
    plt.subplot(1, 2, 1)
    plt.plot(values['accuracy'])
    plt.plot(values['val_accuracy'])
    plt.title('Model Accuracy')
    plt.ylabel('Accuracy')
    plt.xlabel('Epoch')
    plt.legend(['Train', 'Validation'], loc='lower right')
    
    plt.subplot(1, 2, 2)
    plt.plot(values['loss'])
    plt.plot(values['val_loss'])
    plt.title('Model Loss')
    plt.ylabel('Loss')
    plt.xlabel('Epoch')
    plt.legend(['Train', 'Validation'], loc='upper right')
    plt.tight_layout()
    plt.savefig('rnn_training_history.png')
    
    classifier = model
    classifier.save_weights('model/rnnmodel_weights.h5')
    model_json = classifier.to_json()
    with open("model/rnnmodel.json", "w") as json_file:
        json_file.write(model_json)

def runLSTM():
    global lstm_acc
    global X, Y
    global classifier_lstm
    
    early_stopping = EarlyStopping(
        monitor='val_accuracy',
        patience=20,  
        min_delta=0.001,  
        restore_best_weights=True
    )
    
    reduce_lr = ReduceLROnPlateau(
        monitor='val_loss',
        factor=0.2,
        patience=5,
        min_lr=0.0001
    )
    XX = X.reshape((X.shape[0], X.shape[1], 1))
    
    X_train, X_val, Y_train, Y_val = train_test_split(XX, Y, test_size=0.2, random_state=42)
    
    model = models.Sequential([
        keras.layers.LSTM(512, input_shape=(X.shape[1], 1), return_sequences=True),
        layers.Dropout(0.3),
        keras.layers.LSTM(256),
        layers.Dropout(0.3),
        layers.Dense(256, activation='relu'),
        layers.BatchNormalization(),
        layers.Dense(128, activation='relu'),
        layers.Dense(Y.shape[1], activation='softmax')
    ])
    optimizer = Adam(learning_rate=0.001)
    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])
    
    print(model.summary())
    
    history = model.fit(
        X_train, Y_train,
        epochs=1,
        batch_size=64,
        validation_data=(X_val, Y_val),
        callbacks=[reduce_lr]  # Removed early stopping
    )
    
    # Get accuracy from best epoch
    values = history.history
    best_epoch = np.argmax(values['val_accuracy'])
    acc = values['val_accuracy'][best_epoch] * 100
    lstm_acc = acc
    
    # Save model and history
    f = open('model/lstmhistory.pckl', 'wb')
    pickle.dump(values, f)
    f.close()
    
    text.insert(END,'LSTM Prediction Accuracy : '+str(acc)+"\n\n")
    text.insert(END,'Training completed in 1 epochs\n\n')
    
    # Plot training history
    plt.figure(figsize=(10, 6))
    plt.subplot(1, 2, 1)
    plt.plot(values['accuracy'])
    plt.plot(values['val_accuracy'])
    plt.title('Model Accuracy')
    plt.ylabel('Accuracy')
    plt.xlabel('Epoch')
    plt.legend(['Train', 'Validation'], loc='lower right')
    
    plt.subplot(1, 2, 2)
    plt.plot(values['loss'])
    plt.plot(values['val_loss'])
    plt.title('Model Loss')
    plt.ylabel('Loss')
    plt.xlabel('Epoch')
    plt.legend(['Train', 'Validation'], loc='upper right')
    plt.tight_layout()
    plt.savefig('lstm_training_history.png')
    
    classifier_lstm = model
    classifier_lstm.save_weights('model/lstmmodel_weights.h5')
    model_json = classifier_lstm.to_json()
    with open("model/lstmmodel.json", "w") as json_file:
        json_file.write(model_json)

def runFF():
    global ff_acc
    global classifier_ff
    
    early_stopping = EarlyStopping(
        monitor='val_accuracy',
        patience=20,  
        min_delta=0.001, 
        restore_best_weights=True
    )
    
    reduce_lr = ReduceLROnPlateau(
        monitor='val_loss',
        factor=0.2,
        patience=5,
        min_lr=0.0001
    )
    
    X_train, X_val, Y_train, Y_val = train_test_split(X, Y, test_size=0.2, random_state=42)
    
    model = models.Sequential([
        layers.Dense(128, activation='relu', input_shape=(X.shape[1],)),
        layers.BatchNormalization(),
        layers.Dropout(0.3),
        layers.Dense(64, activation='relu'),
        layers.Dropout(0.3),
        layers.Dense(Y.shape[1], activation='softmax')])
    
    optimizer = Adam(learning_rate=0.001)
    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])
    
    print(model.summary())
    
    history = model.fit(
        X_train, Y_train,
        epochs=5,
        batch_size=64,
        validation_data=(X_val, Y_val),
        callbacks=[reduce_lr]  
    )
    
    # Get accuracy from best epoch
    values = history.history
    best_epoch = np.argmax(values['val_accuracy'])
    ff_acc = values['val_accuracy'][best_epoch] * 100
    
    text.insert(END,'Feed Forward Neural Network Prediction Accuracy : '+str(ff_acc)+"\n\n")
    text.insert(END,'Training completed in 5 epochs\n\n')
    
    # Plot training history
    plt.figure(figsize=(10, 6))
    plt.subplot(1, 2, 1)
    plt.plot(values['accuracy'])
    plt.plot(values['val_accuracy'])
    plt.title('Model Accuracy')
    plt.ylabel('Accuracy')
    plt.xlabel('Epoch')
    plt.legend(['Train', 'Validation'], loc='lower right')
    
    plt.subplot(1, 2, 2)
    plt.plot(values['loss'])
    plt.plot(values['val_loss'])
    plt.title('Model Loss')
    plt.ylabel('Loss')
    plt.xlabel('Epoch')
    plt.legend(['Train', 'Validation'], loc='upper right')
    plt.tight_layout()
    plt.savefig('ff_training_history.png')
    
    classifier_ff = model
    classifier_ff.save_weights('model/ffmodel_weights.h5')
    model_json = classifier_ff.to_json()
    with open("model/ffmodel.json", "w") as json_file:
        json_file.write(model_json)

def graph():
    global rnn_acc, lstm_acc, ff_acc
    bars = ['RNN Accuracy','LSTM Accuracy','Feed Forward Accuracy']
    height = [rnn_acc, lstm_acc, ff_acc]
    y_pos = np.arange(len(bars))
    
    plt.figure(figsize=(10, 6))
    plt.bar(y_pos, height, color=['blue', 'green', 'orange'])
    plt.xticks(y_pos, bars)
    plt.ylabel('Accuracy (%)')
    plt.title('Model Accuracy Comparison')
    
    # Add value labels on top of each bar
    for i, v in enumerate(height):
        plt.text(i, v + 1, f"{v:.2f}%", ha='center')
    
    plt.tight_layout()
    plt.savefig('accuracy_comparison.png')
    plt.show()

def evaluate_model():
    # Function to perform k-fold cross validation
    global X, Y
    
    text.delete('1.0', END)
    text.insert(END, "Starting 5-fold cross-validation evaluation...\n\n")
    
    # Define models to evaluate
    models = {
        "RNN": create_rnn_model,
        "LSTM": create_lstm_model,
        "FF": create_ff_model
    }
    
    results = {}
    
    for model_name, model_builder in models.items():
        text.insert(END, f"Evaluating {model_name} model...\n")
        kf = KFold(n_splits=5, shuffle=True, random_state=42)
        fold_accuracies = []
        
        for fold, (train_index, val_index) in enumerate(kf.split(X)):
            X_train, X_val = X[train_index], X[val_index]
            Y_train, Y_val = Y[train_index], Y[val_index]
            
            if model_name == "LSTM":
                X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))
                X_val = X_val.reshape((X_val.shape[0], X_val.shape[1], 1))
            
            model = model_builder()
            
            # Define callbacks
            reduce_lr = ReduceLROnPlateau(
                monitor='val_loss',
                factor=0.2,
                patience=5,
                min_lr=0.0001
            )
            
            # Train model with fixed epochs
            history = model.fit(
                X_train, Y_train,
                epochs=5,
                batch_size=64,
                validation_data=(X_val, Y_val),
                callbacks=[reduce_lr],
                verbose=0
            )
            
            # Get best accuracy
            best_acc = max(history.history['val_accuracy']) * 100
            fold_accuracies.append(best_acc)
            text.insert(END, f"  Fold {fold+1} accuracy: {best_acc:.2f}%\n")
            
        # Calculate average accuracy across folds
        avg_acc = np.mean(fold_accuracies)
        std_acc = np.std(fold_accuracies)
        results[model_name] = avg_acc
        
        text.insert(END, f"{model_name} Cross-Validation Results:\n")
        text.insert(END, f"  Average accuracy: {avg_acc:.2f}% (Â±{std_acc:.2f}%)\n\n")
    
    # Plot results
    plt.figure(figsize=(10, 6))
    bars = list(results.keys())
    height = list(results.values())
    y_pos = np.arange(len(bars))
    
    plt.bar(y_pos, height, color=['blue', 'green', 'orange'])
    plt.xticks(y_pos, bars)
    plt.ylabel('Average Accuracy (%)')
    plt.title('Cross-Validation Model Accuracy Comparison')
    
    # Add value labels on top of each bar
    for i, v in enumerate(height):
        plt.text(i, v + 1, f"{v:.2f}%", ha='center')
    
    plt.tight_layout()
    plt.savefig('cross_validation_comparison.png')
    plt.show()

def create_rnn_model():
    # Function to create RNN model
    model = models.Sequential([
        layers.Dense(512, input_dim=X.shape[1], activation='relu', kernel_initializer="uniform"),
        layers.BatchNormalization(),
        layers.Dropout(0.3),
        layers.Dense(256, activation='relu', kernel_initializer="uniform"),
        layers.Dropout(0.3),
        layers.Dense(128, activation='relu', kernel_initializer="uniform"),
        layers.Dense(Y.shape[1], activation='softmax', kernel_initializer="uniform")
    ])
    
    optimizer = Adam(learning_rate=0.001)
    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])
    
    return model

def create_lstm_model():
    # Function to create LSTM model
    model = models.Sequential([
        keras.layers.LSTM(512, input_shape=(X.shape[1], 1), return_sequences=True),
        layers.Dropout(0.3),
        keras.layers.LSTM(256),
        layers.Dropout(0.3),
        layers.Dense(256, activation='relu'),
        layers.BatchNormalization(),
        layers.Dense(128, activation='relu'),
        layers.Dense(Y.shape[1], activation='softmax')
    ])
    
    optimizer = Adam(learning_rate=0.001)
    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])
    
    return model

def create_ff_model():
    # Function to create Feed Forward model
    model = models.Sequential([
        layers.Dense(128, activation='relu', input_shape=(X.shape[1],)),
        layers.BatchNormalization(),
        layers.Dropout(0.3),
        layers.Dense(64, activation='relu'),
        layers.Dropout(0.3),
        layers.Dense(Y.shape[1], activation='softmax')
    ])
    
    optimizer = Adam(learning_rate=0.001)
    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])
    
    return model

# Modified function for 1-year prediction
def predict_next_1_year():
    global classifier, classifier_lstm, classifier_ff
    global crop_dataset, original_crop_dataset, X, Y
    
    # Check if models are trained
    if 'classifier' not in globals() or classifier is None or 'classifier_lstm' not in globals() or classifier_lstm is None or 'classifier_ff' not in globals() or classifier_ff is None:
        text.delete('1.0', END)
        text.insert(END, "Error: Please train all models first before predicting!\n")
        return
    
    # Open a new window for prediction setup
    predict_window = Toplevel(main)
    predict_window.title("1-Year Crop Yield Prediction")
    predict_window.geometry("700x600")
    
    # Create mapping dictionaries for user-friendly display
    state_mapping = {}
    district_mapping = {}
    crop_mapping = {}
    season_mapping = {}
    
    # Get unique values and create mappings
    if 'original_crop_dataset' in globals():
        unique_states = original_crop_dataset['State_Name'].unique()
        unique_districts = original_crop_dataset['District_Name'].unique()
        unique_crops = original_crop_dataset['Crop'].unique()
        unique_seasons = original_crop_dataset['Season'].unique()
        
        # Create mappings from display name to encoded value
        le_temp = LabelEncoder()
        state_encoded = le_temp.fit_transform(unique_states)
        for i, state in enumerate(unique_states):
            state_mapping[state] = state_encoded[i]
            
        district_encoded = le_temp.fit_transform(unique_districts)
        for i, district in enumerate(unique_districts):
            district_mapping[district] = district_encoded[i]
            
        crop_encoded = le_temp.fit_transform(unique_crops)
        for i, crop in enumerate(unique_crops):
            crop_mapping[crop] = crop_encoded[i]
            
        season_encoded = le_temp.fit_transform(unique_seasons)
        for i, season in enumerate(unique_seasons):
            season_mapping[season] = season_encoded[i]
    else:
        # Fallback to encoded values if original dataset not available
        unique_states = crop_dataset['State_Name'].unique()
        unique_districts = crop_dataset['District_Name'].unique()
        unique_crops = crop_dataset['Crop'].unique()
        unique_seasons = crop_dataset['Season'].unique()
    
    # Frame for inputs
    input_frame = Frame(predict_window)
    input_frame.pack(pady=10)
    
    # Add input fields for prediction
    Label(input_frame, text="Select State:", font=('Arial', 10, 'bold')).grid(row=0, column=0, padx=5, pady=5, sticky=W)
    state_var = StringVar()
    if state_mapping:
        state_var.set(list(state_mapping.keys())[0])
        state_dropdown = ttk.Combobox(input_frame, textvariable=state_var, values=list(state_mapping.keys()), width=20)
    else:
        state_var.set(unique_states[0])
        state_dropdown = ttk.Combobox(input_frame, textvariable=state_var, values=list(unique_states), width=20)
    state_dropdown.grid(row=0, column=1, padx=5, pady=5, sticky=W)
    
    Label(input_frame, text="Select District:", font=('Arial', 10, 'bold')).grid(row=1, column=0, padx=5, pady=5, sticky=W)
    district_var = StringVar()
    if district_mapping:
        district_var.set(list(district_mapping.keys())[0])
        district_dropdown = ttk.Combobox(input_frame, textvariable=district_var, values=list(district_mapping.keys()), width=20)
    else:
        district_var.set(unique_districts[0])
        district_dropdown = ttk.Combobox(input_frame, textvariable=district_var, values=list(unique_districts), width=20)
    district_dropdown.grid(row=1, column=1, padx=5, pady=5, sticky=W)
    
    Label(input_frame, text="Select Crop:", font=('Arial', 10, 'bold')).grid(row=2, column=0, padx=5, pady=5, sticky=W)
    crop_var = StringVar()
    if crop_mapping:
        crop_var.set(list(crop_mapping.keys())[0])
        crop_dropdown = ttk.Combobox(input_frame, textvariable=crop_var, values=list(crop_mapping.keys()), width=20)
    else:
        crop_var.set(unique_crops[0])
        crop_dropdown = ttk.Combobox(input_frame, textvariable=crop_var, values=list(unique_crops), width=20)
    crop_dropdown.grid(row=2, column=1, padx=5, pady=5, sticky=W)
    
    Label(input_frame, text="Select Season:", font=('Arial', 10, 'bold')).grid(row=3, column=0, padx=5, pady=5, sticky=W)
    season_var = StringVar()
    if season_mapping:
        season_var.set(list(season_mapping.keys())[0])
        season_dropdown = ttk.Combobox(input_frame, textvariable=season_var, values=list(season_mapping.keys()), width=20)
    else:
        season_var.set(unique_seasons[0])
        season_dropdown = ttk.Combobox(input_frame, textvariable=season_var, values=list(unique_seasons), width=20)
    season_dropdown.grid(row=3, column=1, padx=5, pady=5, sticky=W)
    
    Label(input_frame, text="Area (hectares):", font=('Arial', 10, 'bold')).grid(row=4, column=0, padx=5, pady=5, sticky=W)
    area_var = DoubleVar()
    area_var.set(100.0)
    area_entry = Entry(input_frame, textvariable=area_var, width=22)
    area_entry.grid(row=4, column=1, padx=5, pady=5, sticky=W)
    
    Label(input_frame, text="Rainfall (mm):", font=('Arial', 10, 'bold')).grid(row=5, column=0, padx=5, pady=5, sticky=W)
    rainfall_var = DoubleVar()
    rainfall_var.set(800.0)
    rainfall_entry = Entry(input_frame, textvariable=rainfall_var, width=22)
    rainfall_entry.grid(row=5, column=1, padx=5, pady=5, sticky=W)
    
    # Add text area for results
    result_text = Text(predict_window, height=12, width=80, font=('Arial', 9))
    result_text.pack(pady=10)
    
    # Add a frame for the figure
    fig_frame = Frame(predict_window)
    fig_frame.pack(pady=10, fill=BOTH, expand=True)
    
    def make_prediction():
        # Get user inputs and convert to encoded values
        state_name = state_var.get()
        district_name = district_var.get()
        crop_name = crop_var.get()
        season_name = season_var.get()
        area = area_var.get()
        rainfall = rainfall_var.get()
        
        # Debug: Print the mappings to understand the encoding
        print(f"Selected: {state_name}, {district_name}, {crop_name}, {season_name}")
        
        # Convert to encoded values using the SAME LabelEncoder used during training
        # This is crucial - we need to use the same encoder that was used in preprocessing
        try:
            # Find the encoded values from the processed dataset
            # Get a sample from the original data that matches our selection
            if 'original_crop_dataset' in globals():
                # Find rows that match our selection
                matching_rows = original_crop_dataset[
                    (original_crop_dataset['State_Name'] == state_name) &
                    (original_crop_dataset['District_Name'] == district_name) &
                    (original_crop_dataset['Crop'] == crop_name) &
                    (original_crop_dataset['Season'] == season_name)
                ]
                
                if len(matching_rows) > 0:
                    # Use the first matching row as reference
                    ref_row = matching_rows.iloc[0]
                    
                    # Get the corresponding encoded values from the processed dataset
                    ref_idx = matching_rows.index[0]
                    if ref_idx < len(crop_dataset):
                        state = crop_dataset.iloc[ref_idx]['State_Name']
                        district = crop_dataset.iloc[ref_idx]['District_Name'] 
                        crop = crop_dataset.iloc[ref_idx]['Crop']
                        season = crop_dataset.iloc[ref_idx]['Season']
                    else:
                        # Fallback encoding
                        le_temp = LabelEncoder()
                        state = le_temp.fit_transform([state_name] + list(original_crop_dataset['State_Name'].unique()))[0]
                        district = le_temp.fit_transform([district_name] + list(original_crop_dataset['District_Name'].unique()))[0]
                        crop = le_temp.fit_transform([crop_name] + list(original_crop_dataset['Crop'].unique()))[0]
                        season = le_temp.fit_transform([season_name] + list(original_crop_dataset['Season'].unique()))[0]
                else:
                    # Fallback encoding if no matching rows found
                    le_temp = LabelEncoder()
                    all_states = list(original_crop_dataset['State_Name'].unique())
                    all_districts = list(original_crop_dataset['District_Name'].unique())
                    all_crops = list(original_crop_dataset['Crop'].unique())
                    all_seasons = list(original_crop_dataset['Season'].unique())
                    
                    state = all_states.index(state_name) if state_name in all_states else 0
                    district = all_districts.index(district_name) if district_name in all_districts else 0
                    crop = all_crops.index(crop_name) if crop_name in all_crops else 0
                    season = all_seasons.index(season_name) if season_name in all_seasons else 0
            else:
                # Use mapping if available
                if state_mapping:
                    state = state_mapping.get(state_name, 0)
                    district = district_mapping.get(district_name, 0)
                    crop = crop_mapping.get(crop_name, 0)
                    season = season_mapping.get(season_name, 0)
                else:
                    state = state_name
                    district = district_name
                    crop = crop_name
                    season = season_name
        except Exception as e:
            print(f"Encoding error: {e}")
            state, district, crop, season = 0, 0, 0, 0
        
        print(f"Encoded values: State={state}, District={district}, Crop={crop}, Season={season}")
        
        # Create feature vector similar to training data
        sample = np.zeros(X.shape[1])
        
        # Set the known features based on the preprocessing structure
        sample[0] = state  # State_Name (encoded)
        sample[1] = district  # District_Name (encoded)
        sample[2] = season  # Season (encoded)
        sample[3] = crop  # Crop (encoded)
        sample[4] = area  # Area
        if X.shape[1] > 5:
            sample[5] = rainfall  # Rainfall (if exists in dataset)
        
        # Add derived features
        if X.shape[1] > 6:
            sample[6] = area * season  # Area_Season interaction
        
        # Add polynomial features if they exist in the model
        if X.shape[1] > 7:
            try:
                poly = PolynomialFeatures(2, include_bias=False)
                poly_features = poly.fit_transform(np.array([[area, rainfall]]))
                for i in range(poly_features.shape[1]):
                    if i + 7 < X.shape[1]:
                        sample[i + 7] = poly_features[0, i]
            except:
                # If polynomial features creation fails, fill with zeros
                pass
        
        print(f"Feature vector before scaling: {sample[:10]}")  # Print first 10 features
        
        # Scale the features using the same scaler used during training
        sample = scalerX.transform(sample.reshape(1, -1))
        
        print(f"Feature vector after scaling: {sample[0][:10]}")  # Print first 10 features
        
        # For LSTM, reshape input
        sample_lstm = sample.reshape(1, sample.shape[1], 1)
        
        # Make predictions for next year (2026)
        prediction_year = 2026
        
        # Get high/low yield probability from each model
        rnn_pred = classifier.predict(sample, verbose=0)[0]
        lstm_pred = classifier_lstm.predict(sample_lstm, verbose=0)[0]
        ff_pred = classifier_ff.predict(sample, verbose=0)[0]
        
        print(f"RNN predictions: {rnn_pred}")
        print(f"LSTM predictions: {lstm_pred}")
        print(f"FF predictions: {ff_pred}")
        
        # Improved yield calculation with more variation
        # Use the actual probability values instead of just binary classification
        base_yield = 2500  # Reduced base yield for more realistic values
        
        # Calculate yield using probability-weighted approach
        rnn_yield = base_yield + (rnn_pred[1] - rnn_pred[0]) * 1000  # Scale by 1000
        lstm_yield = base_yield + (lstm_pred[1] - lstm_pred[0]) * 1000
        ff_yield = base_yield + (ff_pred[1] - ff_pred[0]) * 1000
        
        # Add some variation based on area and rainfall
        area_factor = min(max(area / 100.0, 0.5), 2.0)  # Area factor between 0.5 and 2.0
        rainfall_factor = min(max(rainfall / 800.0, 0.7), 1.5)  # Rainfall factor between 0.7 and 1.5
        
        # Apply environmental factors
        rnn_yield = max(rnn_yield * area_factor * rainfall_factor, 1000)
        lstm_yield = max(lstm_yield * area_factor * rainfall_factor, 1000)
        ff_yield = max(ff_yield * area_factor * rainfall_factor, 1000)
        
        # Add crop-specific modifiers
        crop_modifiers = {
            'Rice': 1.1, 'Wheat': 1.0, 'Sugarcane': 1.5, 'Cotton': 0.8,
            'Maize': 1.2, 'Jowar': 0.9, 'Bajra': 0.85, 'Groundnut': 0.7
        }
        
        crop_modifier = crop_modifiers.get(crop_name, 1.0)
        rnn_yield *= crop_modifier
        lstm_yield *= crop_modifier
        ff_yield *= crop_modifier
        
        # Calculate confidence levels
        rnn_confidence = max(rnn_pred) * 100
        lstm_confidence = max(lstm_pred) * 100
        ff_confidence = max(ff_pred) * 100
        
        print(f"Final yields: RNN={rnn_yield:.2f}, LSTM={lstm_yield:.2f}, FF={ff_yield:.2f}")
        
        # Display results in text area
        result_text.delete("1.0", END)
        result_text.insert(END, f"Crop Yield Prediction for Year {prediction_year}\n")
        result_text.insert(END, "=" * 50 + "\n\n")
        
        result_text.insert(END, f"Input Parameters:\n")
        result_text.insert(END, f"State: {state_name} (encoded: {state})\n")
        result_text.insert(END, f"District: {district_name} (encoded: {district})\n")
        result_text.insert(END, f"Crop: {crop_name} (encoded: {crop})\n")
        result_text.insert(END, f"Season: {season_name} (encoded: {season})\n")
        result_text.insert(END, f"Area: {area} hectares\n")
        result_text.insert(END, f"Rainfall: {rainfall} mm\n")
        result_text.insert(END, f"Area Factor: {area_factor:.2f}, Rainfall Factor: {rainfall_factor:.2f}\n")
        result_text.insert(END, f"Crop Modifier: {crop_modifier:.2f}\n\n")
        
        result_text.insert(END, f"Model Probabilities:\n")
        result_text.insert(END, f"RNN: Low={rnn_pred[0]:.3f}, High={rnn_pred[1]:.3f}\n")
        result_text.insert(END, f"LSTM: Low={lstm_pred[0]:.3f}, High={lstm_pred[1]:.3f}\n")
        result_text.insert(END, f"FF: Low={ff_pred[0]:.3f}, High={ff_pred[1]:.3f}\n\n")
        
        result_text.insert(END, f"Model Predictions for {prediction_year}:\n")
        result_text.insert(END, f"RNN Model: {rnn_yield:.2f} kg/hectare (Confidence: {rnn_confidence:.1f}%)\n")
        result_text.insert(END, f"LSTM Model: {lstm_yield:.2f} kg/hectare (Confidence: {lstm_confidence:.1f}%)\n")
        result_text.insert(END, f"Feed Forward Model: {ff_yield:.2f} kg/hectare (Confidence: {ff_confidence:.1f}%)\n\n")
        
        # Calculate average prediction
        avg_yield = (rnn_yield + lstm_yield + ff_yield) / 3
        result_text.insert(END, f"Average Prediction: {avg_yield:.2f} kg/hectare\n\n")
        
        # Add prediction category
        base_comparison = 2500  # Updated base for comparison
        if avg_yield >= base_comparison:
            category = "HIGH YIELD EXPECTED"
            color_cat = "green"
        else:
            category = "LOW YIELD EXPECTED"
            color_cat = "red"
        
        result_text.insert(END, f"Prediction Category: {category}\n")
        result_text.insert(END, f"Expected Production: {avg_yield * area:.2f} kg total\n")
        
        # Plot the predictions
        plt.figure(figsize=(12, 8))
        
        # Create subplot for bar chart
        plt.subplot(2, 1, 1)
        models = ['RNN Model', 'LSTM Model', 'Feed Forward Model', 'Average']
        yields = [rnn_yield, lstm_yield, ff_yield, avg_yield]
        colors = ['blue', 'green', 'orange', 'red']
        
        bars = plt.bar(models, yields, color=colors, alpha=0.7)
        plt.ylabel('Predicted Yield (kg/hectare)')
        plt.title(f'Crop Yield Prediction for {crop_name} in {prediction_year}')
        plt.xticks(rotation=45)
        
        # Add value labels on top of each bar
        for bar, yield_val in zip(bars, yields):
            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 50,
                    f'{yield_val:.0f}', ha='center', va='bottom', fontweight='bold')
        
        # Add horizontal line for base yield
        plt.axhline(y=base_yield, color='black', linestyle='--', alpha=0.5, label=f'Base Yield ({base_yield} kg/ha)')
        plt.legend()
        
        # Create subplot for confidence levels
        plt.subplot(2, 1, 2)
        confidences = [rnn_confidence, lstm_confidence, ff_confidence]
        model_names = ['RNN Model', 'LSTM Model', 'Feed Forward Model']
        conf_colors = ['lightblue', 'lightgreen', 'lightsalmon']
        
        bars2 = plt.bar(model_names, confidences, color=conf_colors, alpha=0.7)
        plt.ylabel('Confidence Level (%)')
        plt.title('Model Confidence Levels')
        plt.xticks(rotation=45)
        plt.ylim(0, 100)
        
        # Add value labels on confidence bars
        for bar, conf_val in zip(bars2, confidences):
            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,
                    f'{conf_val:.1f}%', ha='center', va='bottom', fontweight='bold')
        
        plt.tight_layout()
        
        # Clear previous plot if any
        for widget in fig_frame.winfo_children():
            widget.destroy()
        
        # Display the plot in the GUI
        canvas = FigureCanvasTkAgg(plt.gcf(), master=fig_frame)
        canvas.draw()
        canvas.get_tk_widget().pack(fill=BOTH, expand=True)
        
        # Save the prediction plot
        plt.savefig('1year_prediction.png', dpi=150, bbox_inches='tight')
        
        # Show success message
        result_text.insert(END, "\nPrediction completed successfully!\n")
        result_text.insert(END, "Plot saved as '1year_prediction.png'\n")
    
    # Button to make prediction
    predict_button = Button(input_frame, text="Generate 1-Year Prediction", command=make_prediction, 
                          font=('times', 12, 'bold'), bg='lightgreen', fg='white')
    predict_button.grid(row=6, columnspan=2, padx=5, pady=15)
    
    # Add instructions
    instruction_frame = Frame(predict_window)
    instruction_frame.pack(pady=5)
    
    instruction_label = Label(instruction_frame, 
                            text="Instructions: Select your parameters and click 'Generate 1-Year Prediction'",
                            font=('Arial', 9, 'italic'), fg='gray')
    instruction_label.pack()

# UI setup
font = ('times', 15, 'bold')
title = Label(main, text='Crop Yield Prediction System', justify=LEFT)
title.config(bg='lavender blush', fg='DarkOrchid1')  
title.config(font=font)           
title.config(height=3, width=120)       
title.place(x=100,y=5)
title.pack()

font1 = ('times', 12, 'bold')
uploadButton = Button(main, text="Upload Agriculture Dataset", command=upload, bg='lightblue')
uploadButton.place(x=50,y=100)
uploadButton.config(font=font1)  

preprocessButton = Button(main, text="Preprocess Dataset", command=preprocess, bg='lightcyan')
preprocessButton.place(x=300,y=100)
preprocessButton.config(font=font1)

rnnButton = Button(main, text="Run RNN Algorithm", command=runRNN, bg='lightpink')
rnnButton.place(x=500,y=100)
rnnButton.config(font=font1)

lstmButton = Button(main, text="Run LSTM Algorithm", command=runLSTM, bg='lightgray')
lstmButton.place(x=700,y=100)
lstmButton.config(font=font1)

ffButton = Button(main, text="Run Feedforward Neural Network", command=runFF, bg='lightyellow')
ffButton.place(x=50,y=150)
ffButton.config(font=font1)

graphButton = Button(main, text="Accuracy Comparison Graph", command=graph, bg='lightsteelblue')
graphButton.place(x=300,y=150)
graphButton.config(font=font1)

evalButton = Button(main, text="Cross-Validation Evaluation", command=evaluate_model, bg='thistle')
evalButton.place(x=550,y=150)
evalButton.config(font=font1)

# Updated button for 1-year prediction
predictButton = Button(main, text="Predict Crop Yield for Next Year", command=predict_next_1_year, 
                      bg='lime green', fg='white')
predictButton.place(x=300,y=200)
predictButton.config(font=font1)

font1 = ('times', 12, 'bold')
text=Text(main,height=20,width=160)
scroll=Scrollbar(text)
text.configure(yscrollcommand=scroll.set)
text.place(x=10,y=250)
text.config(font=font1) 

main.config(bg='light coral')
main.mainloop()